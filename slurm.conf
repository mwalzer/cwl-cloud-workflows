#
# Example: deploy a SLURM cluster on Ubuntu 16.04 VMs
#
# This example covers the installation of a SLURM cluster with 4 compute nodes
# and one login/front-end node (which runs SLURM controller daemon, NFS server
# for home directories, and NIS/YP server for accounts). This configuration also
# demoes how to enable add-on services in the cluster:
#
# - the Ganglia monitoring system will be running on every compute node, with
#   metrics collected and graphed on the front-end (open URL
#   http://master001/ganglia)
# - Each compute node is also a GlusterFS "brick" data server and the whole
#   cluster shares a `/glusterfs` file system which stores data across
#   compute nodes.
#
# The configuration is *incomplete* and needs to be complemented with a suitable
# `cloud` section.
#
# For more details about the configuration, see:
# - http://elasticluster.readthedocs.io/en/latest/configure.html
# - http://elasticluster.readthedocs.io/en/latest/playbooks.html#id4
#
[cloud/openstack]
provider=openstack
auth_url=${OS_AUTH_URL}
username=${OS_USERNAME}
password=${OS_PASSWORD}
project_name=${OS_TENANT_NAME}

[setup/slurm]
provider=ansible

master_groups=slurm_master,ganglia_master#,glusterfs_client
worker_groups=slurm_worker,ganglia_monitor#,glusterfs_server
# `submit` nodes are normally not used but are available if
# you want to load-balance SSH access to the cluster
submit_groups=slurm_submit#,glusterfs_client

# note that Ubuntu 16.04 does *not* install `python2.7` by default, so this
# won't work with official Ubuntu images (still, Ansible requires Python 2)
global_var_ansible_python_interpreter=/usr/bin/python2.7

# install NIS/YP to manage cluster users
global_var_multiuser_cluster=yes


# the `login` section collects information about how to log-in to VMs, including
# SSH key to use for connections
[login/ubuntu]
image_user=ubuntu
image_user_sudo=root
image_sudo=True
# name of SSH key the cloud controller will inject into VMs
user_key_name=elasticluster
# these should match the SSH key named above
user_key_private=/.ssh/id_rsa
user_key_public=/.ssh/id_rsa.pub


[cluster/slurm-on-ubuntu16]
setup=slurm
master_nodes=1
worker_nodes=4
ssh_to=master

# this is cloud-specific info (using OpenStack for the example)
cloud=openstack
flavor=s1.small
network_ids=d98d1d7a-8449-4219-a8f6-cba6b8149552
security_group=Allow-SSH
ssh_proxy_command = ssh -i /.ssh/id_rsa -o StrictHostKeyChecking=no -W %%h:%%p -q ubuntu@193.62.55.220

# Ubuntu 16.04 + python2.7
image_id=5d285650-2e3d-4c9c-8212-b47952a2973a

# `login` info is -in theory- image-specific
login=ubuntu